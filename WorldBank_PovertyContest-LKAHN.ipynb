{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Problem Statement: Predict whether a country is poor based on World Bank household survey data for countries\n",
    "#A, B and C - http://drivendata.co/blog/worldbank-poverty-benchmark/\n",
    "\n",
    "#import libraries: dataframe manipulation, machine learning, os tools\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "import matplotlib.pylab as plt\n",
    "import scipy.stats as stats\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#A-training data has 342 features, 3703 poor = True, 4500 poor=False\n",
    "Atrain = pd.read_csv(\"A_hhold_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#B training has 440 features with poor = T 251 instances and poor =F 3004 instances\n",
    "Btrain = pd.read_csv(\"B_hhold_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#C training has 163 features with poor = T 973 instances and poor =F 5496 instances\n",
    "Ctrain = pd.read_csv(\"C_hhold_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbQAAAFJCAYAAADzONiBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xf23f9b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Country A - encode the categorical features as numbers to try to figure out which \n",
    "#features most influence classification of poor or not.\n",
    "\n",
    "#Let's explore the correlation between the different features in the columns.\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing\n",
    "def number_encode_features(Atrain):\n",
    "    result = Atrain.copy()\n",
    "    encoders = {}\n",
    "    for column in result.columns:\n",
    "        if result.dtypes[column] == np.object:\n",
    "            encoders[column] = preprocessing.LabelEncoder()\n",
    "            result[column] = encoders[column].fit_transform(result[column])\n",
    "    return result, encoders\n",
    "\n",
    "# Plot the correlation between features as a heatmap\n",
    "encoded_data, _ = number_encode_features(Atrain)\n",
    "sns.heatmap(Atrain.corr(), square=True)\n",
    "plt.show()\n",
    "\n",
    "#Dimensionality reduction from 343 features to 4 features would be 'nEsgxvAq', 'OmtioXZZ', 'YFMZwKrU' and 'TrwRslOh'\n",
    "#However, let's not throw away any features just yet.\n",
    "\n",
    "#From the correlation plot, it appears that 'nEsgxvAq' is highly positively correlated (0.4) with 'poor'\n",
    "#'OmtioXZZ' is slightly negatively correlated (-0.2), 'YFMZwKrU' is slightly more negatively correlated (-0.3),\n",
    "# and 'TrwRslOh' is negatively correlated (-0.35)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Preprocess the data for country A - convert object types to categoricals\n",
    "# Standardize features\n",
    "def standardize(Atrain, numeric_only=True):\n",
    "    numeric = Atrain.select_dtypes(include=['int64', 'float64'])\n",
    "    \n",
    "    # subtracy mean and divide by std\n",
    "    Atrain[numeric.columns] = (numeric - numeric.mean()) / numeric.std()\n",
    "    \n",
    "    return Atrain\n",
    "    \n",
    "def pre_process_data(Atrain, enforce_cols=None):\n",
    "    print(\"Input shape:\\t{}\".format(Atrain.shape))\n",
    "        \n",
    "    df = standardize(Atrain)\n",
    "    print(\"After standardization {}\".format(Atrain.shape))\n",
    "        \n",
    "    # create dummy variables for categoricals\n",
    "    df = pd.get_dummies(Atrain)\n",
    "    print(\"After converting categoricals:\\t{}\".format(Atrain.shape))\n",
    "    \n",
    "\n",
    "    # match test set and training set columns\n",
    "    if enforce_cols is not None:\n",
    "        to_drop = np.setdiff1d(Atrain.columns, enforce_cols)\n",
    "        to_add = np.setdiff1d(enforce_cols, Atrain.columns)\n",
    "\n",
    "        Atrain.drop(to_drop, axis=1, inplace=True)\n",
    "        df = Atrain.assign(**{c: 0 for c in to_add})\n",
    "    \n",
    "    Atrain.fillna(0, inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country A\n",
      "Input shape:\t(8203, 345)\n",
      "After standardization (8203, 345)\n",
      "After converting categoricals:\t(8203, 345)\n"
     ]
    }
   ],
   "source": [
    "print(\"Country A\")\n",
    "aX_train = pre_process_data(Atrain.drop('poor', axis=1))\n",
    "ay_train = np.ravel(Atrain.poor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>nEsgxvAq</th>\n",
       "      <th>OMtioXZZ</th>\n",
       "      <th>YFMZwKrU</th>\n",
       "      <th>TiwRslOh</th>\n",
       "      <th>country_A</th>\n",
       "      <th>wBXbHZmp_DkQlr</th>\n",
       "      <th>wBXbHZmp_JhtDR</th>\n",
       "      <th>SlDKnCuu_GUusz</th>\n",
       "      <th>SlDKnCuu_alLXR</th>\n",
       "      <th>...</th>\n",
       "      <th>JCDeZBXq_LPtkN</th>\n",
       "      <th>JCDeZBXq_UyAms</th>\n",
       "      <th>HGPWuGlV_WKNwg</th>\n",
       "      <th>HGPWuGlV_vkbkA</th>\n",
       "      <th>GDUPaBQs_qCEuA</th>\n",
       "      <th>GDUPaBQs_qQxrL</th>\n",
       "      <th>WuwrCsIY_AITFl</th>\n",
       "      <th>WuwrCsIY_GAZGl</th>\n",
       "      <th>AlDbXTlZ_aQeIm</th>\n",
       "      <th>AlDbXTlZ_cecIq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.123796</td>\n",
       "      <td>-1.447160</td>\n",
       "      <td>0.325746</td>\n",
       "      <td>1.099716</td>\n",
       "      <td>-0.628045</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.148155</td>\n",
       "      <td>-0.414625</td>\n",
       "      <td>-0.503468</td>\n",
       "      <td>-0.016050</td>\n",
       "      <td>0.713467</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.693262</td>\n",
       "      <td>0.617910</td>\n",
       "      <td>-0.503468</td>\n",
       "      <td>-0.016050</td>\n",
       "      <td>0.713467</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.635970</td>\n",
       "      <td>0.617910</td>\n",
       "      <td>-0.503468</td>\n",
       "      <td>-0.016050</td>\n",
       "      <td>0.266296</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000822</td>\n",
       "      <td>-0.414625</td>\n",
       "      <td>1.154960</td>\n",
       "      <td>1.099716</td>\n",
       "      <td>0.713467</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 860 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  nEsgxvAq  OMtioXZZ  YFMZwKrU  TiwRslOh  country_A  \\\n",
       "0 -0.123796 -1.447160  0.325746  1.099716 -0.628045          1   \n",
       "1  1.148155 -0.414625 -0.503468 -0.016050  0.713467          1   \n",
       "2  1.693262  0.617910 -0.503468 -0.016050  0.713467          1   \n",
       "3  1.635970  0.617910 -0.503468 -0.016050  0.266296          1   \n",
       "4  0.000822 -0.414625  1.154960  1.099716  0.713467          1   \n",
       "\n",
       "   wBXbHZmp_DkQlr  wBXbHZmp_JhtDR  SlDKnCuu_GUusz  SlDKnCuu_alLXR  \\\n",
       "0               0               1               1               0   \n",
       "1               0               1               1               0   \n",
       "2               0               1               0               1   \n",
       "3               0               1               1               0   \n",
       "4               0               1               1               0   \n",
       "\n",
       "        ...        JCDeZBXq_LPtkN  JCDeZBXq_UyAms  HGPWuGlV_WKNwg  \\\n",
       "0       ...                     1               0               0   \n",
       "1       ...                     0               1               0   \n",
       "2       ...                     0               1               0   \n",
       "3       ...                     0               1               0   \n",
       "4       ...                     1               0               0   \n",
       "\n",
       "   HGPWuGlV_vkbkA  GDUPaBQs_qCEuA  GDUPaBQs_qQxrL  WuwrCsIY_AITFl  \\\n",
       "0               1               0               1               1   \n",
       "1               1               0               1               1   \n",
       "2               1               0               1               1   \n",
       "3               1               0               1               1   \n",
       "4               1               0               1               1   \n",
       "\n",
       "   WuwrCsIY_GAZGl  AlDbXTlZ_aQeIm  AlDbXTlZ_cecIq  \n",
       "0               0               1               0  \n",
       "1               0               0               1  \n",
       "2               0               1               0  \n",
       "3               0               0               1  \n",
       "4               0               0               1  \n",
       "\n",
       "[5 rows x 860 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aX_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#TEST DATA - HOUSEHOLD SURVEYS is same as training data ,\n",
    "#except the poor column is not included\n",
    "\n",
    "Atest = pd.read_csv(\"A_hhold_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Btest = pd.read_csv(\"B_hhold_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Ctest = pd.read_csv(\"C_hhold_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Preprocess the data for country A test data - convert object types to categoricals\n",
    "# Standardize features\n",
    "def standardize(result, numeric_only=True):\n",
    "    numeric = result.select_dtypes(include=['int64', 'float64'])\n",
    "    \n",
    "    # subtracy mean and divide by std\n",
    "    Atest[numeric.columns] = (numeric - numeric.mean()) / numeric.std()\n",
    "    \n",
    "    return result\n",
    "    \n",
    "\n",
    "def pre_process_data(result, enforce_cols=None):\n",
    "    print(\"Input shape:\\t{}\".format(result.shape))\n",
    "        \n",
    "\n",
    "    df = standardize(result)\n",
    "    print(\"After standardization {}\".format(result.shape))\n",
    "        \n",
    "    # create dummy variables for categoricals\n",
    "    df = pd.get_dummies(result)\n",
    "    print(\"After converting categoricals:\\t{}\".format(result.shape))\n",
    "    \n",
    "\n",
    "    # match test set and training set columns\n",
    "    if enforce_cols is not None:\n",
    "        to_drop = np.setdiff1d(result.columns, enforce_cols)\n",
    "        to_add = np.setdiff1d(enforce_cols, result.columns)\n",
    "\n",
    "        result.drop(to_drop, axis=1, inplace=True)\n",
    "        df = result.assign(**{c: 0 for c in to_add})\n",
    "    \n",
    "    result.fillna(0, inplace=True)\n",
    "    \n",
    "    return df\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing\n",
    "def number_encode_features(Atest):\n",
    "    result2 = result.copy()\n",
    "    encoders = {}\n",
    "    for column in result2.columns:\n",
    "        if result2.dtypes[column] == np.object:\n",
    "            encoders[column] = preprocessing.LabelEncoder()\n",
    "            result2[column] = encoders[column].fit_transform(result[column])\n",
    "    return result, encoders\n",
    "\n",
    "def standardize(result2, numeric_only=True):\n",
    "    numeric = result2.select_dtypes(include=['int64', 'float64'])\n",
    "    \n",
    "    # subtracy mean and divide by std\n",
    "    result2[numeric.columns] = (numeric - numeric.mean()) / numeric.std()\n",
    "    \n",
    "    return result2\n",
    "    \n",
    "\n",
    "def pre_process_data(result2, enforce_cols=None):\n",
    "    print(\"Input shape:\\t{}\".format(result2.shape))\n",
    "        \n",
    "\n",
    "    df = standardize(result2)\n",
    "    print(\"After standardization {}\".format(result2.shape))\n",
    "        \n",
    "    # create dummy variables for categoricals\n",
    "    df = pd.get_dummies(result2)\n",
    "    print(\"After converting categoricals:\\t{}\".format(result2.shape))\n",
    "    \n",
    "\n",
    "    # match test set and training set columns\n",
    "    if enforce_cols is not None:\n",
    "        to_drop = np.setdiff1d(result2.columns, enforce_cols)\n",
    "        to_add = np.setdiff1d(enforce_cols, result2.columns)\n",
    "\n",
    "        result2.drop(to_drop, axis=1, inplace=True)\n",
    "        df = result2.assign(**{c: 0 for c in to_add})\n",
    "    \n",
    "    result2.fillna(0, inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Preprocess the data for country B test data - convert object types to categoricals\n",
    "# Standardize features\n",
    "def standardize(result, numeric_only=True):\n",
    "    numeric = result.select_dtypes(include=['int64', 'float64'])\n",
    "    \n",
    "    # subtracy mean and divide by std\n",
    "    Btest[numeric.columns] = (numeric - numeric.mean()) / numeric.std()\n",
    "    \n",
    "    return result\n",
    "    \n",
    "\n",
    "def pre_process_data(result, enforce_cols=None):\n",
    "    print(\"Input shape:\\t{}\".format(result.shape))\n",
    "        \n",
    "\n",
    "    df = standardize(result)\n",
    "    print(\"After standardization {}\".format(result.shape))\n",
    "        \n",
    "    # create dummy variables for categoricals\n",
    "    df = pd.get_dummies(result)\n",
    "    print(\"After converting categoricals:\\t{}\".format(result.shape))\n",
    "    \n",
    "\n",
    "    # match test set and training set columns\n",
    "    if enforce_cols is not None:\n",
    "        to_drop = np.setdiff1d(result.columns, enforce_cols)\n",
    "        to_add = np.setdiff1d(enforce_cols, result.columns)\n",
    "\n",
    "        result.drop(to_drop, axis=1, inplace=True)\n",
    "        df = result.assign(**{c: 0 for c in to_add})\n",
    "    \n",
    "    result.fillna(0, inplace=True)\n",
    "    \n",
    "    return df\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing\n",
    "def number_encode_features(Btest):\n",
    "    result2 = result.copy()\n",
    "    encoders = {}\n",
    "    for column in result2.columns:\n",
    "        if result2.dtypes[column] == np.object:\n",
    "            encoders[column] = preprocessing.LabelEncoder()\n",
    "            result2[column] = encoders[column].fit_transform(result[column])\n",
    "    return result, encoders\n",
    "\n",
    "def standardize(result2, numeric_only=True):\n",
    "    numeric = result2.select_dtypes(include=['int64', 'float64'])\n",
    "    \n",
    "    # subtracy mean and divide by std\n",
    "    result2[numeric.columns] = (numeric - numeric.mean()) / numeric.std()\n",
    "    \n",
    "    return result2\n",
    "    \n",
    "\n",
    "def pre_process_data(result2, enforce_cols=None):\n",
    "    print(\"Input shape:\\t{}\".format(result2.shape))\n",
    "        \n",
    "\n",
    "    df = standardize(result2)\n",
    "    print(\"After standardization {}\".format(result2.shape))\n",
    "        \n",
    "    # create dummy variables for categoricals\n",
    "    df = pd.get_dummies(result2)\n",
    "    print(\"After converting categoricals:\\t{}\".format(result2.shape))\n",
    "    \n",
    "\n",
    "    # match test set and training set columns\n",
    "    if enforce_cols is not None:\n",
    "        to_drop = np.setdiff1d(result2.columns, enforce_cols)\n",
    "        to_add = np.setdiff1d(enforce_cols, result2.columns)\n",
    "\n",
    "        result2.drop(to_drop, axis=1, inplace=True)\n",
    "        df = result2.assign(**{c: 0 for c in to_add})\n",
    "    \n",
    "    result2.fillna(0, inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Preprocess the data for country C test data - convert object types to categoricals\n",
    "# Standardize features\n",
    "def standardize(result, numeric_only=True):\n",
    "    numeric = result.select_dtypes(include=['int64', 'float64'])\n",
    "    \n",
    "    # subtracy mean and divide by std\n",
    "    Ctest[numeric.columns] = (numeric - numeric.mean()) / numeric.std()\n",
    "    \n",
    "    return result\n",
    "    \n",
    "\n",
    "def pre_process_data(result, enforce_cols=None):\n",
    "    print(\"Input shape:\\t{}\".format(result.shape))\n",
    "        \n",
    "\n",
    "    df = standardize(result)\n",
    "    print(\"After standardization {}\".format(result.shape))\n",
    "        \n",
    "    # create dummy variables for categoricals\n",
    "    df = pd.get_dummies(result)\n",
    "    print(\"After converting categoricals:\\t{}\".format(result.shape))\n",
    "    \n",
    "\n",
    "    # match test set and training set columns\n",
    "    if enforce_cols is not None:\n",
    "        to_drop = np.setdiff1d(result.columns, enforce_cols)\n",
    "        to_add = np.setdiff1d(enforce_cols, result.columns)\n",
    "\n",
    "        result.drop(to_drop, axis=1, inplace=True)\n",
    "        df = result.assign(**{c: 0 for c in to_add})\n",
    "    \n",
    "    result.fillna(0, inplace=True)\n",
    "    \n",
    "    return df\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing\n",
    "def number_encode_features(Ctest):\n",
    "    result2 = result.copy()\n",
    "    encoders = {}\n",
    "    for column in result2.columns:\n",
    "        if result2.dtypes[column] == np.object:\n",
    "            encoders[column] = preprocessing.LabelEncoder()\n",
    "            result2[column] = encoders[column].fit_transform(result[column])\n",
    "    return result, encoders\n",
    "\n",
    "def standardize(result2, numeric_only=True):\n",
    "    numeric = result2.select_dtypes(include=['int64', 'float64'])\n",
    "    \n",
    "    # subtracy mean and divide by std\n",
    "    result2[numeric.columns] = (numeric - numeric.mean()) / numeric.std()\n",
    "    \n",
    "    return result2\n",
    "    \n",
    "\n",
    "def pre_process_data(result2, enforce_cols=None):\n",
    "    print(\"Input shape:\\t{}\".format(result2.shape))\n",
    "        \n",
    "\n",
    "    df = standardize(result2)\n",
    "    print(\"After standardization {}\".format(result2.shape))\n",
    "        \n",
    "    # create dummy variables for categoricals\n",
    "    df = pd.get_dummies(result2)\n",
    "    print(\"After converting categoricals:\\t{}\".format(result2.shape))\n",
    "    \n",
    "\n",
    "    # match test set and training set columns\n",
    "    if enforce_cols is not None:\n",
    "        to_drop = np.setdiff1d(result2.columns, enforce_cols)\n",
    "        to_add = np.setdiff1d(enforce_cols, result2.columns)\n",
    "\n",
    "        result2.drop(to_drop, axis=1, inplace=True)\n",
    "        df = result2.assign(**{c: 0 for c in to_add})\n",
    "    \n",
    "    result2.fillna(0, inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:6: FutureWarning: using '-' to provide set differences with Indexes is deprecated, use .difference()\n"
     ]
    }
   ],
   "source": [
    "#Country A\n",
    "#Import necessary libraries\n",
    "import sklearn.cross_validation as cross_validation\n",
    "\n",
    "#Define y_train and y_test variables\n",
    "X_atrain, X_atest, y_atrain, y_atest = cross_validation.train_test_split(encoded_data[encoded_data.columns - ['poor']], \n",
    "    encoded_data[\"poor\"], train_size=0.80)\n",
    "\n",
    "#Let's scale the features with mean of 0 and variance of 1 using a Standard Scaler from scikit-learn\n",
    "scaler = preprocessing.StandardScaler()\n",
    "\n",
    "X_atrain = pd.DataFrame(scaler.fit_transform(X_atrain.astype(\"float64\")), columns=X_atrain.columns)\n",
    "X_atest = scaler.transform(X_atest.astype(\"float64\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:6: FutureWarning: using '-' to provide set differences with Indexes is deprecated, use .difference()\n"
     ]
    }
   ],
   "source": [
    "#Country B\n",
    "#Import necessary libraries\n",
    "import sklearn.cross_validation as cross_validation\n",
    "\n",
    "#Define y_train and y_test variables\n",
    "X_btrain, X_btest, y_btrain, y_btest = cross_validation.train_test_split(encoded_data[encoded_data.columns - ['poor']], \n",
    "    encoded_data[\"poor\"], train_size=0.80)\n",
    "\n",
    "#Let's scale the features with mean of 0 and variance of 1 using a Standard Scaler from scikit-learn\n",
    "scaler = preprocessing.StandardScaler()\n",
    "\n",
    "X_btrain = pd.DataFrame(scaler.fit_transform(X_btrain.astype(\"float64\")), columns=X_btrain.columns)\n",
    "X_btest = scaler.transform(X_btest.astype(\"float64\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:6: FutureWarning: using '-' to provide set differences with Indexes is deprecated, use .difference()\n"
     ]
    }
   ],
   "source": [
    "#Country C\n",
    "#Import necessary libraries\n",
    "import sklearn.cross_validation as cross_validation\n",
    "\n",
    "#Define all variables for Country C\n",
    "X_ctrain, X_ctest, y_ctrain, y_ctest = cross_validation.train_test_split(encoded_data[encoded_data.columns - ['poor']], \n",
    "    encoded_data[\"poor\"], train_size=0.80)\n",
    "\n",
    "#Let's scale the features with mean of 0 and variance of 1 using a Standard Scaler from scikit-learn\n",
    "scaler = preprocessing.StandardScaler()\n",
    "\n",
    "X_ctrain = pd.DataFrame(scaler.fit_transform(X_ctrain.astype(\"float64\")), columns=X_ctrain.columns)\n",
    "X_ctest = scaler.transform(X_ctest.astype(\"float64\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(342, 342, 342, 342, 342),\n",
       "       learning_rate='constant', learning_rate_init=0.001, max_iter=1500,\n",
       "       momentum=0.9, nesterovs_momentum=True, power_t=0.5, random_state=1,\n",
       "       shuffle=True, solver='lbfgs', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's try a multiperceptron neural network on Country A training data\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "#Create MLP model with 342 neurons and 4 layers. \n",
    "#Chose 342 since that's the number of features.\n",
    "#Do we need this many neurons for a low mean log loss?\n",
    "mlpatrain= MLPClassifier(solver='lbfgs', hidden_layer_sizes=(342,342,342,342,342), random_state=1,max_iter=1500)\n",
    "#Fit training data to model\n",
    "mlpatrain.fit(X_atrain,y_atrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3636    0]\n",
      " [   0 2926]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       1.00      1.00      1.00      3636\n",
      "       True       1.00      1.00      1.00      2926\n",
      "\n",
      "avg / total       1.00      1.00      1.00      6562\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = mlpatrain.predict(X_atrain)\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(confusion_matrix(y_atrain,predictions))\n",
    "print(classification_report(y_atrain,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country A training log loss: 9.99200722163e-16\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "print(\"Country A training log loss:\", log_loss(y_atrain, mlpatrain.predict(X_atrain)))\n",
    "\n",
    "#Baseline mean log loss = 0.5739 so this is almost perfect!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country A training log loss: 0.063162284962\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "#Create MLP model with 342 neurons and 4 layers. \n",
    "#Chose 342 since that's the number of features.\n",
    "\n",
    "#Let's try a different solver\n",
    "mlpatrain= MLPClassifier(solver='adam', hidden_layer_sizes=(342,342,342,342,342), random_state=1,max_iter=1500)\n",
    "#Fit training data to model\n",
    "mlpatrain.fit(X_atrain,y_atrain)\n",
    "from sklearn.metrics import log_loss\n",
    "print(\"Country A training log loss:\", log_loss(y_atrain, mlpatrain.predict(X_atrain)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country A training log loss: 1.98962994958\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "#Create MLP model with 4 neurons and 10 hidden layers. \n",
    "#Chose 342 since that's the number of features.\n",
    "\n",
    "#Let's try a different solver\n",
    "mlpatrain= MLPClassifier(solver='lbfgs', hidden_layer_sizes=(4,4,4,4,4,4,4,4,4,4,4), random_state=1,max_iter=1500)\n",
    "#Fit training data to model\n",
    "mlpatrain.fit(X_atrain,y_atrain)\n",
    "from sklearn.metrics import log_loss\n",
    "print(\"Country A training log loss:\", log_loss(y_atrain, mlpatrain.predict(X_atrain)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country A training log loss: 0.0105270271849\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "#Create MLP model with 10 neurons and 3 hidden layers. \n",
    "#Chose 342 since that's the number of features.\n",
    "\n",
    "#Choosing number of neurons between 1 and the number of input variables.\n",
    "#Number of layers is 3 (http://www.heatonresearch.com/node/707 for supporting rationale and\n",
    "#Blum, A. (1992), Neural Networks in C++, NY: Wiley. )\n",
    "mlpatrain= MLPClassifier(solver='lbfgs', hidden_layer_sizes=(10,10,10), random_state=1,max_iter=1500,tol=1e-4)\n",
    "#Fit training data to model\n",
    "mlpatrain.fit(X_atrain,y_atrain)\n",
    "from sklearn.metrics import log_loss\n",
    "print(\"Country A training log loss:\", log_loss(y_atrain, mlpatrain.predict(X_atrain)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country A training log loss: 0.0315814471128\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "#Create MLP model with 7 neurons and 3 hidden layers. \n",
    "#Chose 342 since that's the number of features.\n",
    "\n",
    "#Choosing number of neurons between 1 and the number of input variables.\n",
    "#Number of layers is 3 (http://www.heatonresearch.com/node/707 for supporting rationale and\n",
    "#Blum, A. (1992), Neural Networks in C++, NY: Wiley. )\n",
    "mlpatrain= MLPClassifier(solver='lbfgs', hidden_layer_sizes=(7,7,7), random_state=1,max_iter=1500,tol=1e-4)\n",
    "#Fit training data to model\n",
    "mlpatrain.fit(X_atrain,y_atrain)\n",
    "from sklearn.metrics import log_loss\n",
    "print(\"Country A training log loss:\", log_loss(y_atrain, mlpatrain.predict(X_atrain)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country A training log loss: 0.152643539192\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "#Create MLP model with 7 neurons and 3 hidden layers. \n",
    "#Chose 342 since that's the number of features.\n",
    "\n",
    "#Choosing number of neurons between 1 and the number of input variables.\n",
    "#Number of layers is 3 (http://www.heatonresearch.com/node/707 for supporting rationale and\n",
    "#Blum, A. (1992), Neural Networks in C++, NY: Wiley. )\n",
    "mlpatrain= MLPClassifier(solver='lbfgs', hidden_layer_sizes=(7,7,7,7,7), random_state=1,max_iter=1500,tol=1e-4)\n",
    "#Fit training data to model\n",
    "mlpatrain.fit(X_atrain,y_atrain)\n",
    "from sklearn.metrics import log_loss\n",
    "print(\"Country A training log loss:\", log_loss(y_atrain, mlpatrain.predict(X_atrain)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country A training log loss: 1.33693464583\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "#Create MLP model with 5 neurons and 3 hidden layers. \n",
    "#Chose 342 since that's the number of features.\n",
    "\n",
    "#Choosing number of neurons between 1 and the number of input variables.\n",
    "#Number of layers is 3 (http://www.heatonresearch.com/node/707 for supporting rationale and\n",
    "#Blum, A. (1992), Neural Networks in C++, NY: Wiley. )\n",
    "mlpatrain= MLPClassifier(solver='lbfgs', hidden_layer_sizes=(5,5,5), random_state=1,max_iter=1500,tol=1e-4)\n",
    "#Fit training data to model\n",
    "mlpatrain.fit(X_atrain,y_atrain)\n",
    "from sklearn.metrics import log_loss\n",
    "print(\"Country A training log loss:\", log_loss(y_atrain, mlpatrain.predict(X_atrain)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country A training log loss: 0.0105269053322\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "#Create MLP model with 5 neurons and 3 hidden layers. \n",
    "#Chose 342 since that's the number of features.\n",
    "\n",
    "#Let's try a different solver\n",
    "mlpatrain= MLPClassifier(solver='lbfgs', hidden_layer_sizes=(15,15,15), random_state=1,max_iter=1500)\n",
    "#Fit training data to model\n",
    "mlpatrain.fit(X_atrain,y_atrain)\n",
    "from sklearn.metrics import log_loss\n",
    "print(\"Country A training log loss:\", log_loss(y_atrain, mlpatrain.predict(X_atrain)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[901   0]\n",
      " [  0 740]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       1.00      1.00      1.00       901\n",
      "       True       1.00      1.00      1.00       740\n",
      "\n",
      "avg / total       1.00      1.00      1.00      1641\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Let's try a multiperceptron neural network on Country A test data\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "#Create MLP model with 342 neurons and 4 layers. (#features/3)\n",
    "mlpatest= MLPClassifier(solver='lbfgs', hidden_layer_sizes=(342,342,342,342,342), random_state=1,max_iter=1500)\n",
    "#Fit training data to model\n",
    "mlpatest.fit(X_atest,y_atest)\n",
    "\n",
    "predictions = mlpatest.predict(X_atest)\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(confusion_matrix(y_atest,predictions))\n",
    "print(classification_report(y_atest,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country A test log loss: 9.99200722163e-16\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "print(\"Country A test log loss:\", log_loss(y_atest, mlpatest.predict(X_atest)))\n",
    "\n",
    "#Baseline mean log loss = 0.5739 so the A test results are much worse than the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3586    0]\n",
      " [   0 2976]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       1.00      1.00      1.00      3586\n",
      "       True       1.00      1.00      1.00      2976\n",
      "\n",
      "avg / total       1.00      1.00      1.00      6562\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Country B training data Multi-layer perceptron\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "#Create MLP model with 440 neurons  (country B has 440 features) and 4 layers. (#features/3)\n",
    "mlpbtrain= MLPClassifier(solver='lbfgs', hidden_layer_sizes=(440,440,440,440), random_state=1,max_iter=500)\n",
    "#Fit training data to model\n",
    "mlpbtrain.fit(X_btrain,y_btrain)\n",
    "predictions = mlpbtrain.predict(X_btrain)\n",
    "\n",
    "#Get the metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(confusion_matrix(y_btrain,predictions))\n",
    "print(classification_report(y_btrain,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country B training log loss: 9.99200722163e-16\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "print(\"Country B training log loss:\", log_loss(y_btrain, mlpbtrain.predict(X_btrain)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[914   0]\n",
      " [  0 727]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       1.00      1.00      1.00       914\n",
      "       True       1.00      1.00      1.00       727\n",
      "\n",
      "avg / total       1.00      1.00      1.00      1641\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Country B test data Multi-layer perceptron\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "#Create MLP model with 440 neurons  (country B has 440 features) and 4 layers. (#features/3)\n",
    "mlpbtest= MLPClassifier(solver='lbfgs', hidden_layer_sizes=(440,440,440,440), random_state=1,max_iter=500)\n",
    "#Fit training data to model\n",
    "mlpbtest.fit(X_btest,y_btest)\n",
    "predictions = mlpbtest.predict(X_btest)\n",
    "\n",
    "#Get the metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(confusion_matrix(y_btest,predictions))\n",
    "print(classification_report(y_btest,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country B test log loss: 9.99200722163e-16\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "print(\"Country B test log loss:\", log_loss(y_btest, mlpbtest.predict(X_btest)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3595    0]\n",
      " [   0 2967]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       1.00      1.00      1.00      3595\n",
      "       True       1.00      1.00      1.00      2967\n",
      "\n",
      "avg / total       1.00      1.00      1.00      6562\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Country C training data Multi-layer perceptron\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "#Create MLP model with 163 neurons  (country C has 163 features) and 4 layers. (#features/3)\n",
    "mlpctrain= MLPClassifier(solver='lbfgs', hidden_layer_sizes=(163,163,163,163), random_state=1,max_iter=500)\n",
    "#Fit training data to model\n",
    "mlpctrain.fit(X_ctrain,y_ctrain)\n",
    "predictions = mlpctrain.predict(X_ctrain)\n",
    "\n",
    "#Get the metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(confusion_matrix(y_ctrain,predictions))\n",
    "print(classification_report(y_ctrain,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country C training log loss: 9.99200722163e-16\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "print(\"Country C training log loss:\", log_loss(y_ctrain, mlpctrain.predict(X_ctrain)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[513 389]\n",
      " [414 325]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.55      0.57      0.56       902\n",
      "       True       0.46      0.44      0.45       739\n",
      "\n",
      "avg / total       0.51      0.51      0.51      1641\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Country C test data Multi-layer perceptron\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "#Create MLP model with 163 neurons  (country C has 163 features) and 4 layers. (#features/3)\n",
    "mlpctest= MLPClassifier(solver='lbfgs', hidden_layer_sizes=(163,163,163,163), random_state=1,max_iter=500)\n",
    "#Fit training data to model\n",
    "mlpctest.fit(X_ctest,y_ctest)\n",
    "predictions = mlpctrain.predict(X_ctest)\n",
    "\n",
    "#Get the metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(confusion_matrix(y_ctest,predictions))\n",
    "print(classification_report(y_ctest,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country C test log loss: 9.99200722163e-16\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "print(\"Country C test log loss:\", log_loss(y_ctest, mlpctest.predict(X_ctest)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Just to see how it compares to the ANN, \n",
    "#let's try a support vector machine to see if we can get a better handle on the data\n",
    "from sklearn.svm import SVC\n",
    "clf = SVC()\n",
    "clf.fit(X_atrain,y_atrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[781 115]\n",
      " [630 115]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.55      0.87      0.68       896\n",
      "       True       0.50      0.15      0.24       745\n",
      "\n",
      "avg / total       0.53      0.55      0.48      1641\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = clf.predict(X_atest)\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(confusion_matrix(y_atest,predictions))\n",
    "print(classification_report(y_atest,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.6803658549\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "print(log_loss(y_atest, clf.predict(X_atest)))\n",
    "\n",
    "#Log loss is way worse so it's best to stick with the ANNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>country</th>\n",
       "      <th>poor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8827</th>\n",
       "      <td>6775</td>\n",
       "      <td>C</td>\n",
       "      <td>9.992007e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8828</th>\n",
       "      <td>88300</td>\n",
       "      <td>C</td>\n",
       "      <td>9.992007e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8829</th>\n",
       "      <td>35424</td>\n",
       "      <td>C</td>\n",
       "      <td>9.992007e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8830</th>\n",
       "      <td>81668</td>\n",
       "      <td>C</td>\n",
       "      <td>9.992007e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8831</th>\n",
       "      <td>98377</td>\n",
       "      <td>C</td>\n",
       "      <td>9.992007e-16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id country          poor\n",
       "8827   6775       C  9.992007e-16\n",
       "8828  88300       C  9.992007e-16\n",
       "8829  35424       C  9.992007e-16\n",
       "8830  81668       C  9.992007e-16\n",
       "8831  98377       C  9.992007e-16"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.read_csv(\"WB_LKahn3.csv\")\n",
    "submission.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
